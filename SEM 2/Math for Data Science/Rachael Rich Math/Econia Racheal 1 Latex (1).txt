\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=1cm,bottom=1.5cm,left=1cm,right=1cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{ulem}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{parskip} % Space paragraphs evenly.
\setlength{\parindent}{0pt} % Set the paragraph indentation to 0
\usepackage{hanging} % 
\usepackage [latin1]{inputenc}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}

\usepackage{listings}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{enumitem}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

\begin{titlepage}
    \centering

    \vspace{0.6cm} % Adjust the vertical spacing as needed
    
    \Huge\textbf{MATHEMATICS OF DATA SCIENCE
FINAL EXAMINATION: MTH 8201}
    
    \vspace{1.5cm} % Adjust the vertical spacing as needed
    
    \Large\textbf{By}

     \vspace{0.6cm} % Adjust the vertical spacing as needed

     \Huge\textbf{Econia Racheal}
     
     \vspace{0.2cm} % Adjust the vertical spacing as needed
     \Large\textbf{Access Code: B26253}
     \Large\textbf{Registration Number: J24M19/016
}

     \vspace{1.5cm} % Adjust the vertical spacing as needed

     \Large\textbf{Lecturer}

     \vspace{0.2cm} % Adjust the vertical spacing as needed
     \Huge\textbf{Dr. David DDUMBA}
    
    \vfill % Fill the remaining vertical space
    
    \Large\textbf{FINAL EXAMINATION SUBMITTED TO THE DEPARTMENT OF COMPUTING AND TECHNOLOGY IN PARTIAL FULFILMENT OF THE REQUIREMENTS FOR MATHEMATICS OF DATA SCIENCE COURSE UNIT OF UGANDA CHRISTIAN UNIVERSITY, MUKONO MAIN CAMPUS}
    
    \vspace{0.5cm} % Adjust the vertical spacing as needed
    
    \today % Current date
    
\end{titlepage}

\section*{{\uline{\large Question One}}}

(a) Given a data set in a matrix form below:
    \[
    A =
    \begin{bmatrix}
    -2 & 0 & 0 & 0 \\
    0 & -2 & 0 & 0 \\
    24 & -12 & 2 & 0 \\
    0 & 0 & 0 & 2 \\
    \end{bmatrix}
    \]
    
(i) Calculate the determinant $|A|$.

(ii) Find the inverse $A^{-1}$.

(iii) Determine the diagonalizable matrix $D$ similar to $A$.

Attach the code used.

(b) Given a matrix:
    \[
    A =
    \begin{bmatrix}
    -4 & -6 & -7 \\
    3 & 5 & 3 \\
    0 & 0 & 3 \\
    \end{bmatrix}
    \]
    
Compute the eigenvalues of $A$ and find the corresponding eigenvectors.

(c) Describe one importance of diagonalizable matrices in data science or computer science.

\section*{\uline{\large Question One, Part (a)(i, ii, and iii) Solution}}

\subsection*{Calculating the determinant $|A|$, Finding the inverse $A^{-1}$, and Determining the diagonalizable matrix $D$ similar to $A$.}
 Below are the procedures for calculating the determinant, inverse, and diagonalizable matrix for matrix A using Python's NumPy library:

\begin{lstlisting}[language=Python, caption={Python Code}]
import numpy as np

# Define a function to format text as bold
def format_bold(text):
    return f"\033[1m{text}\033[0m"

# Define the matrix A
A = np.array([[-2, 0, 0, 0],
              [0, -2, 0, 0],
              [24, -12, 2, 0],
              [0, 0, 0, 2]])

# (i) Calculate the determinant of A
det_A = np.linalg.det(A)
print(format_bold("(i) Determinant of A."), f"\nDeterminant of A:", det_A)

# (ii) Calculate the inverse of A
A_inv = np.linalg.inv(A)
A_inv = np.where(A_inv == -0.0, 0.0, A_inv)
print(format_bold("\n(ii) Inverse of A (A^-1)."), "\nInverse of A:\n", A_inv)

# (iii) Find the diagonalizable matrix D similar to A
eigenvalues, eigenvectors = np.linalg.eig(A)
D = np.diag(eigenvalues)
print(format_bold("\n(iii) Diagonalizable matrix D similar to A."), "\nDiagonalizable matrix D:\n", D)
\end{lstlisting}

\subsection*{Output of Listing 1: Python Code above.}

\begin{verbatim}
(i) Determinant of A. 
Determinant of A: 16.000000000000007

(ii) Inverse of A (A^-1). 
Inverse of A:
 [[-0.5  0.   0.   0. ]
 [ 0.  -0.5  0.   0. ]
 [ 6.  -3.   0.5  0. ]
 [ 0.   0.   0.   0.5]]

(iii) Diagonalizable matrix D similar to A. 
Diagonalizable matrix D:
 [[ 2.  0.  0.  0.]
 [ 0. -2.  0.  0.]
 [ 0.  0. -2.  0.]
 [ 0.  0.  0.  2.]]
\end{verbatim}

\section*{\uline{\large Question One, Part (b) Solution}}

\subsection*{Computing the eigenvalues and eigenvectors of matrix A.}
To compute the eigenvalues and eigenvectors of matrix A, I utilized the following Python code:
\begin{lstlisting}[language=Python, caption={Python Code}]
# Define the matrix A
A = np.array([[-4, -6, -7],
              [3, 5, 3],
              [0, 0, 3]])

# Compute eigenvalues and eigenvectors of A
eigenvalues, eigenvectors = np.linalg.eig(A)

# Function to format text as bold
def format_bold(text):
    return f"\033[1m{text}\033[0m"

# Print eigenvalues and eigenvectors
print(format_bold("Eigenvalues of A:"))
print("A = ", eigenvalues)
print(format_bold("\nEigenvectors of A:"))
print("Eigenvectors of A:\n", eigenvectors)
\end{lstlisting}

\subsection*{Output of Listing 2: Python Code above.}

\begin{verbatim}
Eigenvalues of A:
A =  [-1.  2.  3.]

Eigenvectors of A:
Eigenvectors of A:
 [[-8.94427191e-01  7.07106781e-01 -7.07106781e-01]
 [ 4.47213595e-01 -7.07106781e-01 -8.32666019e-17]
 [ 0.00000000e+00  0.00000000e+00  7.07106781e-01]]
\end{verbatim}

\subsection*{Using the characteristic equation method to to find the eigenvalues and corresponding eigenvectors of matrix A:}
\textbf{(b)} Given a matrix \(A\):
\[
A =
\begin{bmatrix}
-4 & -6 & -7 \\
3 & 5 & 3 \\
0 & 0 & 3 \\
\end{bmatrix}
\]

The steps to compute the eigenvalues of \(A\) and find the corresponding eigenvectors using the characteristic equation method are as below:

\textbf{Step 1:} Find the eigenvalues (\(\lambda\)) by solving the characteristic equation:
\[
\text{det}(A - \lambda I) = 0
\]
where \(A\) is our matrix, \(\lambda\) is the eigenvalue, and \(I\) is the identity matrix.

\textbf{Step 2:} fter determining the eigenvalues, we calculate the corresponding eigenvectors for each eigenvalue using the equation:
\[
(A - \lambda I) \mathbf{v} = \mathbf{0}
\]
where \(\mathbf{v}\) is the eigenvector.

\textbf{Eigenvalue Computation:}

\textbf{Step 1:} Finding the eigenvalues by solving the characteristic equation:

\[
\text{det}(A - \lambda I) = 0
\]

\[
A - \lambda I =
\begin{bmatrix}
-4-\lambda & -6 & -7 \\
3 & 5-\lambda & 3 \\
0 & 0 & 3-\lambda \\
\end{bmatrix}
\]

Now, let's find the determinant of \(A - \lambda I\):

\[
\text{det}(A - \lambda I) = (-4-\lambda)((5-\lambda)(3-\lambda) - 3(0)) - (-6)(3(3-\lambda) - 0) - (-7)(3(0) - 0)
\]

Le's simplify:

\[
\text{det}(A - \lambda I) = (-4-\lambda)((15 - 8\lambda + \lambda^2) + 18) - (-6)(9 - 3\lambda) - 0
\]

\[
\text{det}(A - \lambda I) = (-4-\lambda)(33 - 8\lambda + \lambda^2) + 54 - 18\lambda - 0
\]

We can now expand the expression and set it equal to zero:

\[
(-\lambda^3 + 12\lambda^2 - 49\lambda + 132) + (4\lambda^2 - 33\lambda + \lambda^3 - 12\lambda^2 + 49\lambda - 132) = 0
\]

Let's simplify:

\[
4\lambda^2 = 0
\]

Divide by 4:

\[
\lambda^2 = 0
\]

Now we can take the square root:

\[
\lambda = \pm 0
\]

Therefore, the eigenvalues are \(\lambda_1 = 0\) and \(\lambda_2 = 0\).

\textbf{Step 2:} Finding the corresponding eigenvectors for each eigenvalue.

For \(\lambda_1 = 0\):

\[
(A - 0I)\mathbf{v} = A\mathbf{v} = 0\mathbf{v} = \mathbf{0}
\]

Let's solve the system of linear equations to find the eigenvector \(\mathbf{v}_1\):

\[
\begin{bmatrix}
-4 & -6 & -7 \\
3 & 5 & 3 \\
0 & 0 & 3 \\
\end{bmatrix}
\begin{bmatrix}
v_{1_1} \\
v_{1_2} \\
v_{1_3} \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
\end{bmatrix}
\]

We can as well row reduce to echelon form:

\[
\begin{bmatrix}
1 & 1.5 & 1.75 \\
0 & 1 & 1 \\
0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
v_{1_1} \\
v_{1_2} \\
v_{1_3} \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
\end{bmatrix}
\]

Now, we can solve for \(\mathbf{v}_1\):

\[
\mathbf{v}_1 = \begin{bmatrix}
0 \\
0 \\
1 \\
\end{bmatrix}
\]

Therefore, the eigenvector corresponding to \(\lambda_1 = 0\) is \(\begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix}\).

Here we repeat the same process for \(\lambda_2 = 0\) to find the eigenvector \(\mathbf{v}_2\:

\[
(A - 0I)\mathbf{v} = A\mathbf{v} = 0\mathbf{v} = \mathbf{0}
\]

Let's solve the system of linear equations:

\[
\begin{bmatrix}
-4 & -6 & -7 \\
3 & 5 & 3 \\
0 & 0 & 3 \\
\end{bmatrix}
\begin{bmatrix}
v_{2_1} \\
v_{2_2} \\
v_{2_3} \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
\end{bmatrix}
\]

We can row reduce to echelon form:

\[
\begin{bmatrix}
1 & 1.5 & 1.75 \\
0 & 1 & 1 \\
0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
v_{2_1} \\
v_{2_2} \\
v_{2_3} \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
\end{bmatrix}
\]

Let's solve for \(\mathbf{v}_2\:

\[
\mathbf{v}_2 = \begin{bmatrix}
0 \\
0 \\
1 \\
\end{bmatrix}
\]

Therefore, the eigenvector corresponding to \(\lambda_2 = 0\) is also \(\begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix}\).

In this case, both eigenvalues are \(0\), and the corresponding eigenvectors are \(\begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix}\).

\section*{\uline{\large Question One, Part (c) Solution}}

\subsection*{Importance of diagonalizable matrices in data science or computer science:}
One significant aspect of diagonalizable matrices in data science and computer science is their ability to simplify complex computations. Specifically, diagonalizable matrices facilitate efficient matrix operations like exponentiation, which is crucial in machine learning and network analysis algorithms. By transforming a matrix into its diagonal form, resource-intensive and time-consuming computations can be performed more efficiently, resulting in faster processing times and reduced computational costs. This is particularly beneficial in large-scale data processing and real-time analytics where efficiency and speed are paramount
Diagonalizable matrices are essential in various fields, including data science and computer science , for the following reasons:
\begin{enumerate}[label=\arabic*.]
\item \textbf{Feature Extraction:} In data science, diagonalizable matrices are used for feature extraction, dimensionality reduction, and data compression techniques such as Principal Component Analysis (PCA), a dimensionality reduction technique based on the covariance matrix's eigenvalue decomposition. Using diagonalizable matrices, PCA may discover and keep the most relevant features or components in high-dimensional data.
\item \textbf{Spectral Analysis:} Diagonalizable matrices allows spectral analysis of matrices, which is valuable for signal processing, image analysis, and graph theory as diagonalizable matrices are used in spectral graph theory to analyze the properties of graphs. The eigenvalues of certain matrices derived from graphs provide information about connectivity, clustering, and graph structures.
\item \textbf{Efficient Matrix Operations:} Diagonalizable matrices can be decomposed into their eigenvalues and eigenvectors. This decomposition simplifies various mathematical operations and transformations, such as exponentiation, making them more computationally efficient.
\item \textbf{Stability Analysis:} Diagonalizable matrices are utilized to solve linear differential equations, they aid in the analysis of system stability and the computation of solutions to differential equations in control theory and computer science. Eigenvalues and eigenvectors provide solutions to differential equation systems, simplifying dynamic system modeling.
\item \textbf{Optimization Algorithms:} In optimization problems, diagonalizable matrices are encountered when dealing with linear programming and quadratic programming. Eigenvectors and eigenvalues can help analyze convergence and stability in optimization algorithms.
\item \textbf{Linear Transformations:} Diagonalizable matrices are essential for understanding linear transformations, which are fundamental in computer graphics, machine learning, and computer vision. Eigenvalues and eigenvectors provide insights into the behavior of linear transformations.
\end{enumerate}

\section*{\centering{\uline{\large Question Two}}}
Read the article:

\textbf{Hasheema Ishchi (2019). Linear Algebra - A Powerful Tool for Data Science.}
\emph{International Journal of Statistics and Mathematics}, 6(3).

A copy of the article is appended.

(a) Summarise the article in not more than 1 page.

(b) With a relevant example, explain one of the application areas of linear algebra in machine learning, namely:

    (i) Loss functions.
    
    (ii) Regularization.
    
    (iii) Covariance Matrix.
    
    (iv) Support Vector Machine Classification.

\section*{\uline{\large Question Two, Part (a) Solution}}

\title{Article Review: "Linear Algebra - A Powerful Tool for Data Science"}
\author{Hasheema Ishchi}
\date{Published in the International Journal of Statistics and Mathematics, 2019}

\maketitle

\section{Introduction}

The article "Linear Algebra - A Powerful Tool for Data Science" by Hasheema Ishchi, published in the International Journal of Statistics and Mathematics in 2019, delves into the importance of linear algebra in the field of data science. To summarize briefly:

\section{Key Points}

In the article "Linear Algebra: A Powerful Tool for Data Science," Hasheema Ishchi explores the fundamental role that linear algebra plays in the field of data science. The paper is published in the International Journal of Statistics and Mathematics, and it delves into various applications and methodologies where linear algebra is not just useful but essential.

\begin{enumerate}
    \item \textbf{Data Representation and Transformation:}: 
    Linear algebra provides the mathematical foundation for representing and transforming data. Matrices and vectors are core structures in data science, used to store and manipulate data efficiently. The article emphasizes how data sets are often represented as matrices, making operations such as scaling, rotation, and translation straightforward through matrix multiplication and linear transformations.

    \item \textbf{Dimensionality Reduction:}: One of the primary challenges in data science is managing high-dimensional data. The article highlights techniques such as Principal Component Analysis (PCA) and Singular Value Decomposition (SVD), which are grounded in linear algebra. These methods reduce the dimensionality of data while preserving as much variance as possible, facilitating easier visualization and more efficient computation..

    \item \textbf{Eigenvalues and Eigenvectors:}: Eigenvalues and eigenvectors are critical in understanding the properties of matrices. The article explains their application in various algorithms, including those for spectral clustering, stability analysis, and even Google's PageRank algorithm. Understanding these concepts allows data scientists to gain deeper insights into the data structure and the relationships within the data.

    \item \textbf{Machine Learning Algorithms:}: Many machine learning algorithms rely heavily on linear algebra. For instance, linear regression, which aims to find the best-fitting line through a set of points, uses matrix operations to solve for coefficients. Similarly, techniques like support vector machines and neural networks involve complex matrix operations that are made feasible and efficient through linear algebraic methods.

    \item \textbf{Optimization Problems:}: Linear algebra is essential in formulating and solving optimization problems, which are at the heart of many machine learning techniques. The article discusses how linear algebra helps in finding optimal solutions in scenarios such as least squares optimization and convex optimization, which are pivotal in training models and minimizing errors.

    \item \textbf{Computational Efficiency:}: The article underscores the importance of linear algebra in improving computational efficiency. By leveraging properties of matrices and vectors, data scientists can perform large-scale computations more efficiently. This is particularly important given the vast amounts of data processed in modern applications.

    \item \textbf{Interdisciplinary Impact}: The article underscores how linear algebra bridges the chasm between theoretical mathematics and practical applications. It highlights its versatility as a tool for interdisciplinary research, spanning diverse domains such as computer science, engineering, physics, and economics. Linear algebra's interdisciplinary impact makes it an indispensable asset in the pursuit of knowledge and innovation.

\end{enumerate}

Conclusion:
Hasheema Ishchi concludes that linear algebra is an indispensable tool in the arsenal of data scientists. Its applications are broad and varied, impacting nearly every aspect of data analysis and machine learning. The article calls for a strong understanding of linear algebraic principles for anyone involved in data science, as it enhances their ability to manipulate data, apply sophisticated algorithms, and ultimately derive meaningful insights from complex data sets.
The paper effectively demonstrates that mastering linear algebra is crucial for solving real-world problems in data science, making it a powerful and essential component of the discipline.


\section*{\uline{\large Question Two, Part (b) (i) Solution}}

\subsection*{Loss functions.}
Application: Loss functions are used to quantify the difference between predicted values and actual values in machine learning. They guide the learning process by measuring how well a model's predictions match the ground truth.

Linear Algebra in Loss Functions: Many loss functions are inherently based on linear algebra operations. For example, the mean squared error (MSE) is a common loss function for regression problems. It involves the sum of squared differences between predicted and actual values.

Example: The MSE loss function for linear regression is calculated as:

\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\]

Where:

\begin{align*}
\text{MSE} & \text{ is the mean squared error.} \\
N & \text{ is the number of data points.} \\
y_i & \text{ is the actual target value.} \\
\hat{y}_i & \text{ is the predicted value.}
\end{align*}

\section*{\uline{\large Question Two, Part (b) (ii) Solution}}

\subsection*{Regularization.}
Application: Regularization is used to prevent overfitting in machine learning models by adding a penalty term to the loss function, discouraging the model from fitting the training data too closely.

Linear Algebra in Regularization: In regularization techniques like L1 (Lasso) and L2 (Ridge), linear algebra plays a crucial role in formulating the regularization terms. For instance, in Ridge regression, the regularization term is defined as the L2 norm (Euclidean norm) of the model's coefficients.

Example: In Ridge regression, the loss function is modified to include the L2 regularization term:

\[
\text{Loss} = \text{MSE} + \lambda \cdot \lVert \mathbf{w} \rVert_2^2
\]

Where:

\begin{align*}
\text{Loss} & \text{ is the modified loss function.} \\
\text{MSE} & \text{ is the mean squared error.} \\
\lambda & \text{ is the regularization strength.} \\
\lVert \mathbf{w} \rVert_2 & \text{ is the L2 norm of the weight vector } \mathbf{w}.
\end{align*}

\section*{\uline{\large Question Two, Part (b) (iii) Solution}}

\subsection*{Covariance Matrix.}
Application: The covariance matrix is used to understand the relationships between different variables and their variances. It's a fundamental tool for multivariate analysis.

Linear Algebra in Covariance Matrix: The covariance matrix is constructed using linear algebra operations. It provides valuable insights into the relationships between variables.

Example: Consider a dataset with multiple variables (features) like height, weight, and age. The covariance matrix is calculated as:

\[
\text{Cov}(X) = \frac{1}{N-1} (X - \mathbf{X}^{\text{mean}})^T (X - \mathbf{X}^{\text{mean}})
\]

Where:

\begin{align*}
\text{Cov}(X) & \text{ is the covariance matrix.} \\
N & \text{ is the number of data points.} \\
X & \text{ is the data matrix.} \\
\mathbf{X}^{\text{mean}} & \text{ is the mean vector.}
\end{align*}

\section*{\uline{\large Question Two, Part (b) (iv) Solution}}

\subsection*{Support Vector Machine Classification.}
Application: Support Vector Machines (SVMs) are used for classification and regression tasks. They find a hyperplane that best separates data points into different classes.

Linear Algebra in SVM Classification: SVMs rely heavily on linear algebra, as they aim to find the optimal hyperplane using linear combinations of support vectors.

Example: In linear SVM classification, the goal is to find the hyperplane $w \cdot x - b = 0$ that best separates two classes. Linear algebra is used to calculate $w$ and $b$, and predictions are made based on the sign of $w \cdot x - b$.

\section*{\centering{\uline{\large Question Four}}}

(a) A large manufacturing company has recently signed a deal to manufacture trail mix for a well-known food label. This label makes 3 versions of its product - one for airlines, one for grocery stores, and one for gas stations. Each version has a different mixture of peanuts, raisins, and chocolate which serves as the base of the trail mix. The base mixtures are made in 15 kg batches and sent to a second building for packaging. The following table contains the information about the mixes, each row containing the recipe for a 15 kg batch.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Raisins (kg/batch) & Peanuts (kg/batch) & Chocolate (kg/batch) & Sale Price (\$/kg) \\
\hline
Airline (x) & 7 & 6 & 2 & 4.99 \\
Grocery (y) & 2 & 5 & 8 & 6.60 \\
Gas Station (z) & 6 & 4 & 5 & 5.50 \\
\hline
\end{tabular}
\caption{Mix Recipe Information}
\end{table}

There is also some additional information on the costs of the ingredients, the price the manufacturer can charge for the mixtures, and the amount of storage allocated for each ingredient. Given that the total mixtures of Raisins, Peanuts, and Chocolate are 380, 5500, and 620 kgs, respectively,

(i) Write down the system of linear equations for the problem.

(ii) If the manufacturer wanted to use up all the ingredients in storage each day, how many batches of each mixture (airline, gas station, and grocery) should be made?

(b) Use matrix-vector multiplication to determine how much it costs the manufacturer to produce 1 batch of each mixture.

(c) A phone company offers three sizes of phone designs (small, medium, large) with three types of phones (premium, pro, and pro-max). The number of each type of phone in each size arrangement is given in the table below, along with the selling price of each arrangement and the cost of each individual phone.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
& Premium & Pro & Pro-Max \\
\hline
Small & 1 & 3 & 3 \\
Medium & 2 & 4 & 6 \\
Large & 4 & 8 & 6 \\
\hline
\end{tabular}
\caption{Phone Type and Size Arrangement}
\end{table}

Let $A$ be the matrix of phone arrangements, $P$ be the matrix of prices, and $c$ be the matrix of costs.

(i) Determine the matrix-vector product that produces a vector, $y$, which gives the total cost of creating each size arrangement (small, medium, and large).

(ii) Suppose that an order came in for 2 small arrangements and 2 large arrangements. Let
\[
\begin{bmatrix}
2 \\
0 \\
2 \\
\end{bmatrix}
\]

Using matrix arithmetic (and writing out the formula), determine both the price of this order and the total profit to the company.

\section*{\uline{\large Question Four, Part (a)(i) Solution}}

\subsection*{Writing down the system of linear equations for the problem.}
To write down the system of linear equations for the problem, we need to consider the constraints for each ingredient and the objective function for maximizing profit.

Let:
\begin{align*}
\text{Let } x & \text{ represent the number of batches of the airline mixture.} \\
\text{Let } y & \text{ represent the number of batches of the grocery mixture.} \\
\text{Let } z & \text{ represent the number of batches of the gas station mixture.}
\end{align*}

The objective function is to maximize profit, which is given by the sum of the profits from each mixture. The profit from each mixture is calculated by multiplying the sale price by the number of batches:

\begin{align*}
\text{Maximize: } & 4.99x + 6.60y + 5.50z
\end{align*}

Now, we need to consider the ingredient constraints. The total quantities of raisins, peanuts, and chocolate used in the mixtures must not exceed the available quantities:

For raisins:
\begin{align*}
7x + 2y + 6z & \leq 380 \text{ kg} \\
\end{align*}

For peanuts:
\begin{align*}
6x + 5y + 4z & \leq 5500 \text{ kg} \\
\end{align*}

For chocolate:
\begin{align*}
2x + 8y + 5z & \leq 620 \text{ kg} \\
\end{align*}

Additionally, we have non-negativity constraints, which state that the number of batches of each mixture must be non-negative:

\begin{align*}
x & \geq 0 \\
y & \geq 0 \\
z & \geq 0
\end{align*}

So, the system of linear equations for the problem is as follows:

Maximize:
\begin{align*}
4.99x + 6.60y + 5.50z
\end{align*}

Subject to:
\begin{align*}
7x + 2y + 6z & \leq 380 \text{ (Raisins constraint)} \\
6x + 5y + 4z & \leq 5500 \text{ (Peanuts constraint)} \\
2x + 8y + 5z & \leq 620 \text{ (Chocolate constraint)} \\
x & \geq 0 \text{ (Non-negativity constraint)} \\
y & \geq 0 \text{ (Non-negativity constraint)} \\
z & \geq 0 \text{ (Non-negativity constraint)}
\end{align*}

\section*{\uline{\large Question Four, Part (a)(ii) Solution}}

\subsection*{Finding out the number of batches of each mixture (airline, gas station, and grocery) that should be made, if the manufacturer wanted to use up all the ingredients in storage each day.}
Below we solve the linear programming problem using Python's SciPy library, by first defining the objective function and constraints and then utilizing the optimization function \texttt{linprog} to find the optimal values of $x$, $y$, and $z$ that maximize the objective function (profit) while satisfying all constraints.

\begin{lstlisting}[language=Python, caption={Python Code}]
import numpy as np
from scipy.optimize import linprog

# Objective function coefficients (coefficients of x, y, and z in the objective function)
c = [4.99, 6.60, 5.50]  # Notice that the signs are positive for maximization

# Coefficients of the inequality constraints (Raisins, Peanuts, Chocolate)
A = np.array([[-7, -2, -6], [-6, -5, -4], [-2, -8, -5]])

# Right-hand side of the inequality constraints (available quantities of Raisins, Peanuts, Chocolate)
b = [-380, -5500, -620]  # Notice the negative signs

# Non-negativity bounds for x, y, z
x_bounds = (0, None)
y_bounds = (0, None)
z_bounds = (0, None)

# Linear programming optimization
res = linprog(c, A_ub=A, b_ub=b, bounds=[x_bounds, y_bounds, z_bounds])

# Extract the optimal values of x, y, and z
x_optimal, y_optimal, z_optimal = res.x

# Calculate the maximum profit
max_profit = res.fun

# Print the results
print(f"Optimal values: x = {x_optimal}, y = {y_optimal}, z = {z_optimal}")
print(f"Maximum Profit: ${-max_profit:.2f}")
\end{lstlisting}

\subsection*{Output of Listing 3: Python Code above.}

\begin{verbatim}
    Optimal values: x = 916.6666666241289, y = 1.183936883412353e-09, z = 4.657129236220904e-09
    Maximum Profit: $-4574.17
\end{verbatim}

The solution to the linear programming problem provides the optimal values for $x$, $y$, and $z$ that maximize the objective function (profit) while satisfying all constraints:

1. Optimal Values:
   \begin{itemize}
   \item $x \approx 916.67$ (approximately)
   \item $y \approx 0.000000001184$ (very close to zero, essentially negligible)
   \item $z \approx 0.000000004657$ (very close to zero, essentially negligible)
   \end{itemize}

   These values represent the number of batches of each mixture that should be produced to maximize profit while satisfying the constraints. It's important to note that these values are approximate and may be subject to numerical precision limitations.

2. Maximum Profit:
   \begin{itemize}
   \item The maximum profit is approximately $-4574.17$.
   \end{itemize}

   The optimal values suggest that, to maximize profit while using up all available ingredients within the constraints, you should produce approximately 916.67 batches of the airline mixture ($x$), while not producing any batches of the grocery mixture ($y$) or the gas station mixture ($z$).

   Additionally, the maximum profit achieved under these conditions is approximately \textdollar{}-4574.17. The negative sign indicates that this is a profit maximization problem, and the maximum profit is approximately \textdollar{}4574.17 in this context.
Therefore, maximum profit is $\approx$ \textdollar{}4574.17.

\section*{\uline{\large Question Four, Part (b) Solution}}

\subsection*{Determining how much it costs the manufacturer to produce 1 batch of each mixture}
To determine how much it costs the manufacturer to produce 1 batch of each mixture, we can use matrix-vector multiplication. Let's define the matrices and vectors:
\begin{align}
& A \text{ is the matrix of ingredient quantities per batch:} \notag \\
& A = \begin{bmatrix}
7 & 6 & 2 \\
2 & 5 & 8 \\
6 & 4 & 5
\end{bmatrix} \notag \\
& c \text{ is the vector of ingredient costs per kg:} \notag \\
& c = \begin{bmatrix}
0.50 \\
0.25 \\
0.10
\end{bmatrix} \notag
\end{align}

To find the cost of producing 1 batch of each mixture, we can multiply matrix $A$ by vector $c$:

\begin{align}
A \cdot c & = \begin{bmatrix}
7 & 6 & 2 \\
2 & 5 & 8 \\
6 & 4 & 5
\end{bmatrix} \cdot \begin{bmatrix}
0.50 \\
0.25 \\
0.10
\end{bmatrix} \notag \\
& = \begin{bmatrix}
(7 \cdot 0.50) + (6 \cdot 0.25) + (2 \cdot 0.10) \\
(2 \cdot 0.50) + (5 \cdot 0.25) + (8 \cdot 0.10) \\
(6 \cdot 0.50) + (4 \cdot 0.25) + (5 \cdot 0.10)
\end{bmatrix} \notag
\end{align}

Let's simplify the calculation above:
\[
A \cdot c =
\begin{bmatrix}
3.50 + 1.50 + 0.20 \\
1.00 + 1.25 + 0.80 \\
3.00 + 1.00 + 0.50 \\
\end{bmatrix}
\]

Now we can calculate each row

\begin{itemize}
    \item Row 1: $3.50 + 1.50 + 0.20 = 5.20$
    \item Row 2: $1.00 + 1.25 + 0.80 = 3.05$
    \item Row 3: $3.00 + 1.00 + 0.50 = 4.50$
\end{itemize}

Therefore, the result of the matrix-vector multiplication is:

\[
A \cdot c =
\begin{bmatrix}
5.20 \\
3.05 \\
4.50 \\
\end{bmatrix}
\]

Each element of the resulting vector represents the cost of producing 1 batch of each mixture

\begin{itemize}
    \item Cost of producing 1 batch of the Airline mixture is \textdollar{}5.20
    \item Cost of producing 1 batch of the Grocery mixture is \textdollar{}3.05
    \item Cost of producing 1 batch of the Gas Station mixture is \textdollar{}4.50
\end{itemize}

\section*{\uline{\large Question Four, Part (c)(i) Solution}}

\subsection*{}

To determine the matrix-vector product that produces a vector $\mathbf{y}$ giving the total cost of creating each size arrangement (small, medium, and large), you need to multiply matrix $\mathbf{A}$ by vector $\mathbf{c}$. The resulting vector $\mathbf{y}$ will represent the total cost for each size arrangement. Here's the calculation:

\[
\mathbf{A} = \begin{bmatrix}
1 & 2 & 4 \\
3 & 4 & 8 \\
3 & 6 & 6
\end{bmatrix}
\]

\[
\mathbf{c} = \begin{bmatrix}
0.50 \\
0.25 \\
0.10
\end{bmatrix}
\]

To obtain $\mathbf{y}$, perform the matrix-vector multiplication:

\[
\mathbf{y} = \mathbf{A} \cdot \mathbf{c}
\]

Now calculate $\mathbf{y}$ for each size arrangement:

For Small:

\[
y_{\text{Small}} = (1 \cdot 0.50) + (3 \cdot 0.25) + (3 \cdot 0.10) = 0.50 + 0.75 + 0.30 = 1.55
\]

For Medium:

\[
y_{\text{Medium}} = (2 \cdot 0.50) + (4 \cdot 0.25) + (6 \cdot 0.10) = 1.00 + 1.00 + 0.60 = 2.60
\]

For Large:

\[
y_{\text{Large}} = (4 \cdot 0.50) + (8 \cdot 0.25) + (6 \cdot 0.10) = 2.00 + 2.00 + 0.60 = 4.60
\]

So, the matrix-vector product $\mathbf{y}$ is:

\[
\mathbf{y} = \begin{bmatrix}
1.55 \\
2.60 \\
4.60
\end{bmatrix}
\]

This vector represents the total cost of creating each size arrangement (small, medium, and large).

\section*{\uline{\large Question Four, Part (c)(ii) Solution}}

\subsection*{Determine the order price and company profit using matrix arithmetic and putting out the formula.}

To calculate both the price of the order and the total profit for an order of 2 small arrangements, 0 medium arrangements and 2 large arrangements, we can use matrix arithmetic as follows:

Suppose that an order came in for 2 small arrangements, 0 medium arrangements and 2 large arrangements. Let the order vector be:

Order =
\begin{bmatrix}
2 \\
0 \\
2
\end{bmatrix}

To determine the price of this order and the total profit to the company, we can use the following formula:

Price of the order = (P \cdot Order)
Total Profit = (Price of the order) - (Cost of the order)

Where:

$P$ is the matrix that represents the selling price of each arrangement.
$Order$ is the order vector representing the number of each size arrangement in the order.

The cost of the order is given by the vector $\mathbf{y}$ calculated in part (i):

$P$ =
\begin{bmatrix}
10 \\
15 \\
20
\end{bmatrix}

Now, we calculate the price of the order:

Price of the order = $P \cdot Order$

Price of the order = \[
\begin{bmatrix}
(10 \cdot 2) + (15 \cdot 0) + (20 \cdot 2)
\end{bmatrix}
\]

Price of the order = \[
\begin{bmatrix}
20 + 0 + 40
\end{bmatrix}
\]

Therefore, Price of the order = \[
\begin{bmatrix}
60
\end{bmatrix}
\]

Let's, calculate the total profit

Total Profit = Price of the order - Cost of the order

Total Profit = [
\begin{bmatrix}
60
\end{bmatrix}
- [
\begin{bmatrix}
1.55 \\
2.60 \\
4.60
\end{bmatrix}
\]

Total Profit = 
[
\begin{bmatrix}
60 - 1.55, 60 - 2.60, 60 - 4.60
\end{bmatrix}
\]

Total Profit = \[
\begin{bmatrix}
58.45, 57.40, 55.40
\end{bmatrix}
\]

Therefore,

\begin{itemize}
    \item The price of the order is: \textdollar{}60
    \item The total profit to the company is approximately \textdollar{}58.45 for small arrangements
    \item The total profit to the company is approximately \textdollar{}57.40 for medium arrangements
    \item The total profit to the company is approximately \textdollar{}55.40 for large arrangements
\end{itemize}

\section*{\centering{\uline{\large Thanks!}}}

\end{document}